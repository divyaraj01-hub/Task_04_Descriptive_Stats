# -*- coding: utf-8 -*-
"""2024_fb_ads_president_scored_anon .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_7viMYkhQ_3JlGVo1Fdlb6jCLsXs0YJA
"""

import pandas as pd

path = "/content/2024_fb_ads_president_scored_anon.csv"

df = pd.read_csv(path, on_bad_lines='skip', engine='python')  # skip problematic lines
df.head()

import csv
import statistics
from collections import defaultdict, Counter
# Removed pandas import as we are aiming for pure Python processing

def is_number(val):
    """Checks if a value can be converted to a float."""
    try:
        float(val)
        return True
    except (ValueError, TypeError):
        return False

def summarize_column(col_values):
    """Summarizes a list of values, handling both numeric and categorical data."""
    numeric_values = [float(v) for v in col_values if is_number(v)]

    if numeric_values:
        # Summarize numeric data
        summary = {
            "count": len(numeric_values),
            "mean": round(statistics.mean(numeric_values), 2) if numeric_values else 0,
            "min": min(numeric_values) if numeric_values else "N/A",
            "max": max(numeric_values) if numeric_values else "N/A",
        }
        if len(numeric_values) > 1:
            try:
                summary["std_dev"] = round(statistics.stdev(numeric_values), 2)
            except statistics.StatisticsError:
                summary["std_dev"] = "N/A"
        else:
            summary["std_dev"] = "N/A"
        return summary
    else:
        # Summarize categorical data
        # Ensure all values are strings for consistent counting
        col_values_str = [str(v) if v is not None else 'None' for v in col_values]
        return {
            "unique": len(set(col_values_str)),
            "most_common": Counter(col_values_str).most_common(1)[0] if col_values_str else ("N/A", 0)
        }

def summarize_dataset(data_list):
    """Summarizes data provided as a list of dictionaries (rows)."""
    print("\n--- Overall Summary ---")
    if not data_list:
        print("No data to summarize.")
        return

    # Get column names from the first dictionary (assuming consistent keys)
    columns = data_list[0].keys()

    for col in columns:
        # Extract values for the current column across all rows
        col_values = [row.get(col) for row in data_list]
        print(f"\nColumn: {col}")
        for k, v in summarize_column(col_values).items():
            print(f"  {k}: {v}")

def group_by(data_list, keys):
    """Groups data (list of dictionaries) by specified keys."""
    grouped = defaultdict(list)
    for row in data_list:
        try:
            # Create a tuple key from the values of the specified keys
            key = tuple(row.get(k) for k in keys)
            grouped[key].append(row)
        except KeyError as e:
            print(f"Warning: Missing key '{e}' in a row. Skipping row.")
            continue
    return grouped

# === MAIN ===
# The data was loaded into a Polars DataFrame 'df' in a previous cell.
# Convert the Polars DataFrame to a list of dictionaries.
# Use the 'df' variable that exists from the previous successful execution.
data_list = df.to_dicts()


summarize_dataset(data_list)

print("\n--- Grouped by page_id ---")
group1 = group_by(data_list, ["page_id"])
# Iterate through the first 2 groups and summarize
for i, (key, group_data) in enumerate(list(group1.items())):
    if i >= 2: break
    print(f"\nGroup: {key}")
    summarize_dataset(group_data)

print("\n--- Grouped by (page_id, ad_id) ---")
group2 = group_by(data_list, ["page_id", "ad_id"])
# Iterate through the first 2 groups and summarize
for i, (key, group_data) in enumerate(list(group2.items())):
    if i >= 2: break
    print(f"\nGroup: {key}")
    summarize_dataset(group_data)

import pandas as pd

# Load the dataset
df = pd.read_csv("2024_fb_ads_president_scored_anon.csv")

# ===== Overall Descriptive Statistics =====
print("\n--- Descriptive Statistics ---")
print(df.describe(include='all'))

# ===== Unique Counts and Most Frequent Values =====
print("\n--- Unique Count and Most Frequent Values ---")
for col in df.columns:
    print(f"\nColumn: {col}")
    print(f"  Unique values: {df[col].nunique()}")
    if df[col].dtype == 'object':
        print(f"  Most frequent: {df[col].value_counts().idxmax()} ({df[col].value_counts().max()} times)")

# ===== Grouped by 'page_id' =====
numeric_cols = df.select_dtypes(include='number').columns

print("\n--- Grouped by page_id ---")
grouped_page = df.groupby("page_id")[numeric_cols].agg(['count', 'mean', 'min', 'max', 'std'])
print(grouped_page.head(3))

print("\n--- Grouped by (page_id, ad_id) ---")
grouped_page_ad = df.groupby(["page_id", "ad_id"])[numeric_cols].agg(['count', 'mean', 'min', 'max', 'std'])
print(grouped_page_ad.head(3))

import polars as pl

# Load dataset using Polars
df = pl.read_csv("2024_fb_ads_president_scored_anon.csv")

# Print general summary
print("\n--- Descriptive Statistics ---")
print(df.describe())

# Unique + Most frequent
print("\n--- Unique Count and Most Frequent Values ---")
for col in df.columns:
    print(f"\nColumn: {col}")
    print(f"  Unique: {df[col].n_unique()}")
    try:
        most_common = df.group_by(col).count().sort("count", descending=True)[0, col]
        print(f"  Most Frequent: {most_common}")
    except:
        print("  Most Frequent: N/A")

# Select only numeric columns
numeric_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype in [pl.Float64, pl.Int64]]

# Grouped by page_id
print("\n--- Grouped by page_id ---")
grouped_page = df.group_by("page_id").agg([
    pl.count(),
    *[pl.mean(col).alias(f"{col}_mean") for col in numeric_cols],
    *[pl.min(col).alias(f"{col}_min") for col in numeric_cols],
    *[pl.max(col).alias(f"{col}_max") for col in numeric_cols],
    *[pl.std(col).alias(f"{col}_std") for col in numeric_cols],
])
print(grouped_page.head(3))

# Grouped by page_id + ad_id
print("\n--- Grouped by (page_id, ad_id) ---")
grouped_page_ad = df.group_by(["page_id", "ad_id"]).agg([
    pl.count(),
    *[pl.mean(col).alias(f"{col}_mean") for col in numeric_cols],
    *[pl.min(col).alias(f"{col}_min") for col in numeric_cols],
    *[pl.max(col).alias(f"{col}_max") for col in numeric_cols],
    *[pl.std(col).alias(f"{col}_std") for col in numeric_cols],
])
print(grouped_page_ad.head(3))

"""FB Posts

"""

